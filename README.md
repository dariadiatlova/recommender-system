# Тестовое задание на летнюю стажировку в комаду CoreML Вконтакте


Задание: на датасете [MovieLens20M](https://www.kaggle.com/grouplens/movielens-20m-dataset/code)
сравнить два подхода к построению рекомендации:

- коллаборативный: используя только рейтинги. Например SVD-like алгоритмы, ALS, Implicit-ALS.

- коллаборативный + контентный: используя рейтинги и всю дополнительную информацию о фильмах, имеющуюся в датасете. Например LightFM.

Задачи:
1. Выбрать метрику и обосновать выбор.
2. Придумать и обосновать способ разбиения данных на обучение и валидацию.
3. Обратить внимание на сходимость обучения и настройку важных гиперпараметров моделей.
4. Оценить статистическую значимость результатов.

___

## Коллаборативный подход:
0. Для начала почистим данные. Скрипт [`filter_data.py`](src/preprocessing/filter_data.py)
   считает, сколько составляет 1% от всех пользователей и удаляет из датасета
   фильмы с меньшим числом оценок. Размер исходного датасета после фильтрации сократился на 15%. 
   Этот шаг позволяет нам снизить разреженность матрицы рейтингов фильмов и пользователей. 
   

1. Для того, чтобы грамотно выбрать метрику необходимо уточнить задачу, которую мы решаем. 
   Пусть мы хотим, чтобы наша рекомендательная система на этапе `evaluation` выдавала каждому пользователю N рекомендаций фильмов. 
   Переведем пятибалльную рейтинговую шкалу в бинарную, в которой оценки выше или равные 4 будут соответствовать метке `1` – фильм понравился, 
   а остальные оценки метке `0` – фильм не понравился. 
   
Посчитаем несколько метрик:
      
   - `precision@k`;
   - `MAP`.

   Выбор данных метрик продиктован задачей – хочется максимизировать 
   выдачу релевантных рекомендаций, поэтому абсолютно все
   предсказания нас не интересуют, фокус на первых k. 
   Метрика `precision@k` как раз показывает долю релевантных рекомендаций в начале списка. 
   

   Матрица рейтингов пользователей и фильмов сильно разреженная, 
   высока вероятность, что на валидационном датасете для каждого пользователя будет небольшое
   число размеченных данных, но подглядывать в тренировочные данные не хочется, 
   так как высока вероятность переобучиться, поэтому k следует взять побольше, ~25.

   Метрика `MAP` – это обобщение `precision@k` на весь датасет, ее будем использовать так как она показывает, на сколько релевантные оценки,
   в среднем, мы выдаем пользователям. 

2. Для разбиения датасета на тренировочный, валидационный и тестовый воспользуемся стратегией [`Temporal Global`](https://arxiv.org/pdf/2007.13237.pdf).
   Разобьем датасет – файл `raiting.csv` на тренировочный и валидационный согласно выбранной дате так, чтобы в тренировочный датасет попали все
   пользовательские действия, которые были совершены до выбранной даты, а в тестовый – оставшиеся. Затем повторим действия для разбиения на тренировочный и валидационный.
   Разбиение файла `raiting.csv` осуществляется с помощью запуска скрипта [`split_dataset.py`](src/preprocessing/split_dataset.py)
   с командной строки. 
   Размер тестового и валидационного датасета определяется случайным образом, но гарантировано попадает в заданный диапазон значений.
   Параметры разбиения заданы в [`util.py`](src/common/util.py). В нашем случае в тестовый датасет попало 49% данных, в валидационный 21% и в тестовый оставшиеся 30%.
